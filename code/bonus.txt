pca_dim: 10 Accuracy: 1.0
pca_dim: 8 Accuracy: 1.0
pca_dim: 7 Accuracy: 1.0
pca_dim: 6 Accuracy: 1.0
pca_dim: 5 Accuracy: 1.0
pca_dim: 4 Accuracy: 0.90625
pca_dim: 3 Accuracy: 0.875
pca_dim: 2 Accuracy: 0.875
pca_dim: 1 Accuracy: 0.6875

I used PCA for dimensionality reduction, so we transform the m x d–dimensional feature set into a new
m x dprime–dimensional feature subspace that has fewer dimensions than the original d–dimensional feature space.

The results are shown above for different d-prime values (i.e 10, 8, 7, 6, 5, 4, 3, 2, 1). And as we can see,
by reducing the dimensionality, the accuracy of the classification decreases and that makes sense, since the model has
fewer feature dimensions than the original and that makes it difficult for the model to classify the classes correctly.

The code is in bonus.py.